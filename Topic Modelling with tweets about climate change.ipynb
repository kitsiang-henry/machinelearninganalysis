{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the Data:** <br>\n",
    "The data consists of `5000` tweets which are about `'Global Warming'`. I have performed `Topic Modelling` on the tweets text and interpreted the topics by analysing the distribution of words in the topics. After interpreting the topics, I further assigned a `'Theme'` to each topic by conducting my own research according to the distributed words. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the data and check basic properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('global_warming_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few rows of data\n",
    "print(data.head())\n",
    "print('----------------------------')\n",
    "print(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the number of observations in data\n",
    "\n",
    "num_of_data = 5000\n",
    "\n",
    "if len(data) == num_of_data:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of duplicate tweets\n",
    "\n",
    "num_of_duplicates = 0\n",
    "for line in data.tweet.duplicated():\n",
    "    if line == True:\n",
    "        num_of_duplicates += 1\n",
    "        \n",
    "print(num_of_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates and print the number of rows in the new set of data\n",
    "\n",
    "new_data = data.drop_duplicates()\n",
    "print(len(new_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Preprocessing functions** <br>\n",
    "#### Remove @mention, URLs, and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stop words list\n",
    "\n",
    "stop_list = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "print(stop_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer/lemmatizer objects\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenized_data = [word_tokenize(line) for line in new_data.tweet]\n",
    "# print(tokenized_data)\n",
    "\n",
    "WNL = WordNetLemmatizer()\n",
    "lemma_data = []\n",
    "\n",
    "for a in tokenized_data:\n",
    "    lemma_data.append([WNL.lemmatize(word) for word in a])\n",
    "    \n",
    "print(lemma_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function created to remove @mentions, www, http URLs and expand n't to not\n",
    "\n",
    "def remove(s):\n",
    "    new_sentence = re.sub(r'@\\w+', '', s)\n",
    "    new_sentence = re.sub(r'http?://\\S+', '', new_sentence)\n",
    "    new_sentence = re.sub(r'www\\S+', '', new_sentence)\n",
    "    return new_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text processing function\n",
    "\n",
    "def process_alltweets(listtweets):\n",
    "    for sentence in listtweets:\n",
    "        new_sentence = sentence.lower()\n",
    "        new_sentence = remove(new_sentence)\n",
    "        sentence_tokens = word_tokenize(new_sentence)\n",
    "        sentence_tokens = [token for token in sentence_tokens if token not in stop_list]\n",
    "        sentence_tokens = [WNL.lemmatize(token) for token in sentence_tokens]\n",
    "        tweets_token.append(' '.join(sentence_tokens))\n",
    "    return tweets_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_token = []\n",
    "file = open('global_warming_tweets.csv', encoding='utf-8')\n",
    "tweets = file.readlines()\n",
    "tweets.pop(0)\n",
    "processed_tweets = process_alltweets(tweets)\n",
    "print(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector object here\n",
    "\n",
    "tweets_corpus = process_alltweets(tweets)\n",
    "cv = CountVectorizer(tweets_corpus)\n",
    "count_vect = cv.fit_transform(processed_tweets)\n",
    "print(count_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary and result of toarray() here \n",
    "\n",
    "vocab = cv.get_feature_names()\n",
    "# print(vocab)\n",
    "print(\"Number of features :\\n\", len(vocab))\n",
    "print('---------------------------')\n",
    "cv_matrix = count_vect.toarray()\n",
    "print(cv_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "\n",
    "vect_df = pd.DataFrame(cv_matrix, columns=vocab)\n",
    "print(vect_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word counts by applying sum()\n",
    "\n",
    "word_counts = vect_df.sum(axis=0)\n",
    "print(word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sort the word counts to determine the words with highest frequency\n",
    "\n",
    "sorted_word_counts = word_counts.sort_values(ascending=False)\n",
    "top_20_words = sorted_word_counts[:20]\n",
    "print(top_20_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 20 words with frequency in descending order**\n",
    "\n",
    "1. climate, 6124\n",
    "2. change, 5572\n",
    "3. global, 5272\n",
    "4. warming, 5108\n",
    "5. rt, 1660\n",
    "6. link, 1642\n",
    "7. via, 856\n",
    "8. new, 526\n",
    "9. snow, 438\n",
    "10. news, 430\n",
    "11. bill, 424\n",
    "12. tcot, 368\n",
    "13. energy, 348\n",
    "14. science, 342\n",
    "15. scientist, 320\n",
    "16. green, 320\n",
    "17. say, 316\n",
    "18. report, 306\n",
    "19. people, 288\n",
    "20. earth, 284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "processed_tweets_token = [word_tokenize(i) for i in processed_tweets]\n",
    "print(processed_tweets_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create gensim dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the tweets.\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_tweets_token)\n",
    "\n",
    "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size:', len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that occur in less than 5 documents.\n",
    "\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "print('Total Vocabulary Size after filters:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bag of words model in gensim and create list of tuples for every doc/tweet containing (wordid, frequency)\n",
    "\n",
    "# Transform dictionary into bag of words vectors\n",
    "\n",
    "corpus_bag_of_words = [dictionary.doc2bow(text) for text in processed_tweets_token]\n",
    "\n",
    "print(corpus_bag_of_words[0])\n",
    "print('\\n')\n",
    "print(processed_tweets_token[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gensim LDA model by using the bag of words created\n",
    "\n",
    "num_topics = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_bag_of_words, num_topics=num_topics, id2word=dictionary, passes=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the LDA model\n",
    "\n",
    "print(type(ldamodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topics with word distribution\n",
    "\n",
    "for num, topic in ldamodel.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    print(str(num) + ': ' + topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refine the results of the topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update stop list which was created previously\n",
    "\n",
    "new_stopwords = ['global', 'warming', '\\'s', 'rt', '..', '...', '``', \"''\", '--', 'link', 'via']\n",
    "stop_list = stopwords.words('english') + list(string.punctuation) + new_stopwords\n",
    "\n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated function, that was created previously, which includes the processing step to expand 'n\\'t' to 'not'\n",
    "\n",
    "def remove(s):\n",
    "    new_sentence = re.sub(r'@\\w+', '', s)\n",
    "    new_sentence = re.sub(r'http?://\\S+', '', new_sentence)\n",
    "    new_sentence = re.sub(r'www\\S+', '', new_sentence)\n",
    "    new_sentence = re.sub(r'n\\'t', 'not', new_sentence) # this is the new line of code\n",
    "    return new_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_alltweets(listtweets):\n",
    "    for sentence in listtweets:\n",
    "        new_sentence = sentence.lower()\n",
    "        new_sentence = remove(new_sentence)\n",
    "        sentence_tokens = word_tokenize(new_sentence)\n",
    "        sentence_tokens = [token for token in sentence_tokens if token not in stop_list]\n",
    "        sentence_tokens = [WNL.lemmatize(token) for token in sentence_tokens]\n",
    "        tweets_token.append(' '.join(sentence_tokens))\n",
    "    return tweets_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_token = []\n",
    "file = open('global_warming_tweets.csv', encoding='utf-8')\n",
    "tweets = file.readlines()\n",
    "tweets.pop(0)\n",
    "processed_tweets = process_alltweets(tweets)\n",
    "print(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "processed_tweets_token = [word_tokenize(i) for i in processed_tweets]\n",
    "print(processed_tweets_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the tweets.\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_tweets_token)\n",
    "\n",
    "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size:', len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words that occur in less than 5 documents.\n",
    "\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "print('Total Vocabulary Size after filters:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bag of words model in gensim and create list of tuples for every doc/tweet containing (wordid, frequency)\n",
    "\n",
    "# Transform dictionary into bag of words vectors\n",
    "\n",
    "corpus_bag_of_words = [dictionary.doc2bow(text) for text in processed_tweets_token]\n",
    "\n",
    "print(corpus_bag_of_words[0])\n",
    "print('\\n')\n",
    "print(processed_tweets_token[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the LDA model\n",
    "\n",
    "print(type(ldamodel))# Create a gensim LDA model by using the bag of words created\n",
    "\n",
    "num_topics = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_bag_of_words, num_topics=num_topics, id2word=dictionary, passes=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the LDA model\n",
    "\n",
    "print(type(ldamodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topics with word distribution\n",
    "\n",
    "for num, topic in ldamodel.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    print(str(num) + ': ' + topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis: Interpretation of topics to assign Themes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic 0 :** <br>\n",
    "- 0.022*\"new\" + 0.020*\"science\" + 0.019*\"energy\" + 0.018*\"obama\" + 0.012*\"earth\" + 0.012*\"cause\" + 0.011*\"study\" + 0.011*\"say\" + 0.011*\"agency\" + 0.010*\"news\" <br>\n",
    "<font color = 'blue'>`Theme`: New Renewable Energy for America Initiative by Barack Obama </font> <br>\n",
    "\n",
    "\n",
    "**Topic 1 :** <br>\n",
    "- 0.020*\"news\" + 0.013*\"un\" + 0.012*\"great\" + 0.010*\"good\" + 0.009*\"science\" + 0.009*\"issue\" + 0.009*\"health\" + 0.008*\"environmental\" + 0.007*\"public\" + 0.007*\"solution\" <br>\n",
    "<font color = 'blue'>`Theme`: UNEP : United Nations Environment Programme </font> <br>\n",
    "\n",
    "**Topic 2 :** <br>\n",
    "- 0.014*\"new\" + 0.014*\"scientist\" + 0.014*\"report\" + 0.013*\"u.s.\" + 0.010*\"help\" + 0.009*\"allergy\" + 0.009*\"blame\" + 0.008*\"call\" + 0.008*\"india\" + 0.007*\"tcot\" <br>\n",
    "<font  color = 'blue'>`Theme`: The United States and India - Moving Forward Together on Climate Change </font><br>\n",
    "\n",
    "**Topic 3 :** <br>\n",
    "- 0.038*\"bill\" + 0.014*\"graham\" + 0.013*\"law\" + 0.012*\"stop\" + 0.011*\"california\" + 0.011*\"state\" + 0.010*\"senate\" + 0.009*\"get\" + 0.009*\"put\" + 0.008*\"senator\" <br>\n",
    "<font color = 'blue'>`Theme`: Senator Lindsey Graham Pulls Support for Major Senate Climate Bill </font><br>\n",
    "\n",
    "**Topic 4 :** <br>\n",
    "- 0.027*\"snow\" + 0.019*\"people\" + 0.018*\"tcot\" + 0.016*\"gore\" + 0.014*\"dc\" + 0.014*\"al\" + 0.012*\"believe\" + 0.011*\"world\" + 0.009*\"storm\" + 0.009*\"conference\" <br>\n",
    "<font color = 'blue'>`Theme`: Al Gore : An Inconvenient Truth </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of topics based on research**\n",
    "\n",
    "**Topic 0**\n",
    "- Words such as 'new', 'energy', 'obama' and 'earth' indicate that Topic 0 is related to a new energy plan which is initiated by Barack Obama to save the Earth from climate problems. Therefore, Topic 0 can be interpreted as a New Renewable Energy Initiative implemented by Barack Obama to tackle climate change that can be found in news based on some research done. \n",
    "\n",
    "**Topic 1**\n",
    "- 'un' represents the abbreviation of United Nations and the link between 'un' and other words such as 'great', 'issue', 'environmental' and 'public' can lead to the interpretation of Topic 1 as an Environment Programme by United Nations to raise awareness among the public to help in environmental issues. \n",
    "\n",
    "**Topic 2**\n",
    "- As words such as 'u.s.', 'help' and 'india' are distributed to Topic 2, a topic about an opportunity for the United States and India to cooperate in tackling climate change can be interpreted and at the same time, this topic could be a new topic reported by scientists.\n",
    "\n",
    "**Topic 3**\n",
    "- The words distributed to Topic 3 such as 'bill', 'graham', 'law', 'senate' and 'senator' show that Topic 3 could be related to news about law and a bill which has been stopped being supported by a senator named Graham.\n",
    "\n",
    "**Topic 4**\n",
    "- Based on some research done, 'al' and 'gore' appeared to be the name of an environmetalist (Al Gore). Moreover, words such as 'snow', 'world' and 'storm' could be related to climate change which link to a documentary (An Inconvenient Truth) made by the environmentalist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of Topic Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the ldamodel get_document_topics function here and assign the result to document_topics[]\n",
    "\n",
    "document_topics = ldamodel.get_document_topics(corpus_bag_of_words)\n",
    "for i in range(0,10):\n",
    "    print(\"\\n Document :\", i)\n",
    "    print(document_topics[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Perplexity and Coherence Score** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "\n",
    "log_perplexity = ldamodel.log_perplexity(corpus_bag_of_words)\n",
    "perplexity = 2**(-log_perplexity)\n",
    "print('Perplexity: ',perplexity)\n",
    "\n",
    "# Compute Coherence Score\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=processed_tweets_token, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ',coherence_lda)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
